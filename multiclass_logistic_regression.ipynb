{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.6 s, sys: 7.5 s, total: 19.1 s\n",
      "Wall time: 38.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "db = MongoClient('mongodb://143.215.138.132:27017')['big_data']\n",
    "\n",
    "matchNE = {'$match': {'lat': {'$gte': 36, '$lte': 50}, 'lon': {'$gte': -99, '$lte': -69}}}\n",
    "matchSE = {'$match': {'lat': {'$gte': 25, '$lte': 36}, 'lon': {'$gte': -99, '$lte': -69}}}\n",
    "matchNW = {'$match': {'lat': {'$gte': 36, '$lte': 50}, 'lon': {'$gte': -125, '$lte': -99}}}\n",
    "matchSW = {'$match': {'lat': {'$gte': 25, '$lte': 36}, 'lon': {'$gte': -125, '$lte': -99}}}\n",
    "\n",
    "sentence_list = []\n",
    "location_list = []\n",
    "\n",
    "limit = {'$limit': 100000}\n",
    "\n",
    "pipeline = [matchNE, limit]\n",
    "\n",
    "for tweet in db.tweet.aggregate(pipeline):\n",
    "    sentence_list.append(tweet['text'])\n",
    "    location_list.append('NE')\n",
    "\n",
    "pipeline = [matchSE, limit]\n",
    "\n",
    "for tweet in db.tweet.aggregate(pipeline):\n",
    "    sentence_list.append(tweet['text'])\n",
    "    location_list.append('SE')\n",
    "\n",
    "pipeline = [matchNW, limit]\n",
    "\n",
    "for tweet in db.tweet.aggregate(pipeline):\n",
    "    sentence_list.append(tweet['text'])\n",
    "    location_list.append('NW')\n",
    "\n",
    "pipeline = [matchSW, limit]\n",
    "\n",
    "for tweet in db.tweet.aggregate(pipeline):\n",
    "    sentence_list.append(tweet['text'])\n",
    "    location_list.append('SW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add your own sentence here\n",
    "your_sentence = \"Georgia Tech is in Atlanta\"\n",
    "sentence_list.append(your_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Tweet_Tokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = TweetTokenizer()\n",
    "    def __call__(self, doc):\n",
    "        return self.wnl.tokenize(doc)\n",
    "\n",
    "def make_features(corpus):\n",
    "    vectorizer = CountVectorizer(tokenizer=Tweet_Tokenizer(), analyzer='word', min_df=0)\n",
    "    return vectorizer.fit_transform(corpus), vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.8 s, sys: 231 ms, total: 22.1 s\n",
      "Wall time: 22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentence_vector_list, vector_name_list = make_features(sentence_list)\n",
    "your_sentence_vector = sentence_vector_list[-1]\n",
    "sentence_vector_list = sentence_vector_list[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training_vectors, test_vectors, training_locations, test_locations =\\\n",
    "    train_test_split(sentence_vector_list, location_list, test_size=0.1, random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(multi_class='multinomial', solver='saga', max_iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 57s, sys: 665 ms, total: 9min 57s\n",
      "Wall time: 9min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=5000, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='saga',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lr_clf.fit(training_vectors, training_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.58 ms, sys: 2.22 ms, total: 11.8 ms\n",
      "Wall time: 10.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predicted_test_locations = lr_clf.predict(test_vectors)\n",
    "predicted_your_sentence_location = lr_clf.predict(your_sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.4748\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set accuracy: \"\\\n",
    "      + str(accuracy_score(test_locations, predicted_test_locations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of your sentence: ['SE']\n"
     ]
    }
   ],
   "source": [
    "print(\"Result of your sentence: \" + str(predicted_your_sentence_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find Top Weights in Logistic Regression Classifier\n",
    "theta_NE = lr_clf.coef_[0]\n",
    "theta_NW = lr_clf.coef_[1]\n",
    "theta_SE = lr_clf.coef_[2]\n",
    "theta_SW = lr_clf.coef_[3]\n",
    "weights_NE = dict()\n",
    "weights_NW = dict()\n",
    "weights_SE = dict()\n",
    "weights_SW = dict()\n",
    "\n",
    "for feature, weight in zip(vector_name_list, theta_NE):\n",
    "    weights_NE[feature] = weight\n",
    "for feature, weight in zip(vector_name_list, theta_NW):\n",
    "    weights_NW[feature] = weight\n",
    "for feature, weight in zip(vector_name_list, theta_SE):\n",
    "    weights_SE[feature] = weight\n",
    "for feature, weight in zip(vector_name_list, theta_SW):\n",
    "    weights_SW[feature] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Scoring Logistic Regression Weights\n",
      "\n",
      "#toronto\t\tNE\t\t2.90294848331\n",
      "il\t\tNE\t\t2.79604968164\n",
      "#nyc\t\tNE\t\t2.74393936549\n",
      "@ashcrofttom\t\tNE\t\t2.73272379919\n",
      "@benpage11benp\t\tNE\t\t2.67721946977\n",
      "nj\t\tNE\t\t2.67596764088\n",
      "@thsbluedevils\t\tNE\t\t2.65420123949\n",
      "#isles\t\tNE\t\t2.61687329474\n",
      "#wvprepfb\t\tNE\t\t2.61157786911\n",
      "mn\t\tNE\t\t2.46361035804\n",
      "\n",
      "\n",
      "################################\n",
      "\n",
      "\n",
      "#opreps\t\tNW\t\t3.40653149571\n",
      "#sanfrancisco\t\tNW\t\t3.28263404866\n",
      "#copreps\t\tNW\t\t3.2588894379\n",
      "#portland\t\tNW\t\t3.13328463373\n",
      "#seattle\t\tNW\t\t3.10631816\n",
      "https://t.co/gxloesa9oo\t\tNW\t\t2.87680990938\n",
      "#norcalscores\t\tNW\t\t2.80602466871\n",
      "@cloacamaxima01\t\tNW\t\t2.64797156408\n",
      "#mtscores\t\tNW\t\t2.63647972689\n",
      "#goducks\t\tNW\t\t2.58045337141\n",
      "\n",
      "\n",
      "################################\n",
      "\n",
      "\n",
      "@karn33333\t\tSE\t\t3.07310910782\n",
      "@negrosubversive\t\tSE\t\t2.90852836141\n",
      "#tcprepzone\t\tSE\t\t2.73283463734\n",
      "jacksonville\t\tSE\t\t2.66601170378\n",
      "#ghc16\t\tSE\t\t2.62148805616\n",
      "#cubevenue\t\tSE\t\t2.57452245442\n",
      "tx\t\tSE\t\t2.54129372599\n",
      "#wstc\t\tSE\t\t2.50645560027\n",
      "@__kissesileft\t\tSE\t\t2.48434002082\n",
      "#florida\t\tSE\t\t2.48276009965\n",
      "\n",
      "\n",
      "################################\n",
      "\n",
      "\n",
      "#sandiego\t\tSW\t\t3.03520332565\n",
      "https://t.co/pkf3rchefn\t\tSW\t\t2.98341521987\n",
      "#losangeles\t\tSW\t\t2.9608183222\n",
      "az\t\tSW\t\t2.84580962781\n",
      "@brass_tackz\t\tSW\t\t2.63721704898\n",
      "@tanyamics\t\tSW\t\t2.5969676072\n",
      "#th7\t\tSW\t\t2.58005987625\n",
      "#aloween\t\tSW\t\t2.55101605324\n",
      "#elpaso\t\tSW\t\t2.45922763905\n",
      "#ocvupdates\t\tSW\t\t2.44867412736\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Scoring Logistic Regression Weights\\n\")\n",
    "top_features_NE = sorted(weights_NE, key=lambda x:weights_NE[x], reverse=True)[:10]\n",
    "for word in top_features_NE:\n",
    "    print(str(word) + '\\t\\tNE\\t\\t' + str(weights_NE[word]))\n",
    "\n",
    "print('\\n')\n",
    "print('################################')\n",
    "print('\\n')\n",
    "\n",
    "top_features_NW = sorted(weights_NW, key=lambda x:weights_NW[x], reverse=True)[:10]\n",
    "for word in top_features_NW:\n",
    "    print(str(word) + '\\t\\tNW\\t\\t' + str(weights_NW[word]))\n",
    "\n",
    "print('\\n')\n",
    "print('################################')\n",
    "print('\\n')\n",
    "\n",
    "top_features_SE = sorted(weights_SE, key=lambda x:weights_SE[x], reverse=True)[:10]\n",
    "for word in top_features_SE:\n",
    "    print(str(word) + '\\t\\tSE\\t\\t' + str(weights_SE[word]))\n",
    "\n",
    "print('\\n')\n",
    "print('################################')\n",
    "print('\\n')\n",
    "\n",
    "top_features_SW = sorted(weights_SW, key=lambda x:weights_SW[x], reverse=True)[:10]\n",
    "for word in top_features_SW:\n",
    "    print(str(word) + '\\t\\tSW\\t\\t' + str(weights_SW[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find more in Chapter 1 and 2 of the book below\n",
    "https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
